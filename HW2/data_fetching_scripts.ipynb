{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4efa5c7",
   "metadata": {},
   "source": [
    "# HOMEWORK 2 (Data fetching) - Network Measurement and Data Analysis Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d239264",
   "metadata": {},
   "source": [
    "*Stefano Maxenti, 10526141, 970133*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc6f19",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "<a id='data_acquisition'></a>\n",
    "Of the 20 websites, I collect data only from 18 of them using curl. I don't fetch anything from washingtonpost.com and from rt.com: the former because it rejects curl connections (probably by inspecting the user-agent), the latter because curl reaches only an anti-DDoS service page (probably due to cyberwarfare) and is not able to be redirected correctly, thus it would not provide useful insights in the project. \n",
    "\n",
    "To obtain traffic traces, I set up a very small Docker container (\"hw2\") based on Ubuntu on a VPS located in the Netherlands, where I installed **tcpdump** and **curl** and write a small script. The \"-L\" flag in Curl is used to follow redirections, while the \"-4\" to force IPv4.\n",
    "\n",
    "Using a docker container reduces the amount of noise traffic because no other applications are running; in addition to that, I force curl to use a specific range of ephemeral ports (2000-2100) and I filter just on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f8d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1789cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "list=( \"https://www.indiatimes.com\" \"https://www.ndtv.com\" \"https://www.cnbc.com\" \"https://www.timesofindia.com\" \"https://www.express.co.uk\" \"https://www.news18.com\" \"https://www.nypost.com\" \"https://www.abc.net.au\" \"https://www.bbc.co.uk\" \"https://www.msn.com\" \"https://www.cnn.com\" \"https://www.news.google.com\" \"https://www.dailymail.co.uk\" \"https://www.nytimes.com\" \"https://www.theguardian.com\" \"https://www.foxnews.com\" \"https://www.finance.yahoo.com\" \"https://www.news.yahoo.com\" )\r\n",
      "\r\n",
      ". /etc/profile\r\n",
      "\r\n",
      "for i in \"${list[@]}\"\r\n",
      "do\r\n",
      "\ta=$(echo $i|cut -d \".\" -f2,3,4)\r\n",
      "\techo $a\r\n",
      "\t/usr/sbin/tcpdump -i eth0 -w /root/$a-$(date +%Y-%m-%d_%H-%M-%S).pcap portrange 2000-2100 &\r\n",
      "\tsleep 2\r\n",
      "\tcurl -L -4 $i --local-port 2000-2100\r\n",
      "\t#killall curl\r\n",
      "\tsleep 5\r\n",
      "\tpkill tcpdump\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/fetching.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cc00b",
   "metadata": {},
   "source": [
    "With **tshark**, I convert each pcap file to a CSV file.\n",
    "These files are then imported inside the notebook and are available in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8abdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\r\n",
      "\r\n",
      "# This bash script iterates over all pcap files in the same folder.\r\n",
      "# For each one of them, outputs a CSV file with the same name using tshark\r\n",
      "for i in *.pcap\r\n",
      "do\r\n",
      "\t/usr/bin/echo \"Processing \" $i\r\n",
      "\t/usr/bin/tshark -r $i -T fields -e frame.number -e frame.time -e frame.len \\\r\n",
      "\t\t-e ip.len -e ip.src -e ip.dst -e ip.proto \\\r\n",
      "\t\t-e tcp.srcport -e tcp.dstport -e tcp.len -e tcp.option_kind \\\r\n",
      "\t\t\t-E header=y -E separator=, -E quote=d > CSV/$i.csv\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/pcap_to_csv.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d5773",
   "metadata": {},
   "source": [
    "I then set up a crontab entry on the host machine:\n",
    "\n",
    "Fetching happens every ten minutes between 6 and 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6922bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*/10 06-23 * * * docker exec -t hw2 /root/script.sh\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/crontab_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7df617",
   "metadata": {},
   "source": [
    "Unfortunately, curl may not provide a clear representation of real traffic, because it does not download images and does not run javascript. Some more details are provided in the conclusion sections of Biflow and CUMUL approaches.\n",
    "\n",
    "I try another approach: after spinning up a Xubuntu virtual machine in Virtualbox, I collect traces coming from a real browser (Firefox).\n",
    "\n",
    "The automation script is very similar to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e77085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "\n",
      "list=( \"https://www.indiatimes.com\" \"https://www.ndtv.com\" \"https://www.cnbc.com\" \"https://www.timesofindia.com\" \"https://www.express.co.uk\" \"https://www.news18.com\" \"https://www.nypost.com\" \"https://www.abc.net.au\" \"https://www.bbc.co.uk\" \"https://www.msn.com\" \"https://www.cnn.com\" \"https://www.news.google.com\" \"https://www.dailymail.co.uk\" \"https://www.nytimes.com\" \"https://www.theguardian.com\" \"https://www.foxnews.com\" \"https://www.finance.yahoo.com\" \"https://www.news.yahoo.com\" )\n",
      "\n",
      ". /etc/profile\n",
      "\n",
      "for i in \"${list[@]}\"\n",
      "do\n",
      "\ta=$(echo $i|cut -d \".\" -f2,3,4)\n",
      "\techo $a\n",
      "\t/usr/bin/tcpdump -i enp0s3 -w /home/stefano/DATASET_HW2/$a-$(date +%Y-%m-%d_%H-%M-%S).pcap port 443 &\n",
      "\tsleep 2\n",
      "\t/usr/bin/firefox $i &\n",
      "\tsleep 20\n",
      "\twmctrl -c \"Firefox\" -x \"Navigator.Firefox\"\n",
      "\tsleep 2\n",
      "\tpkill firefox\n",
      "\tpkill tcpdump\n",
      "done\n",
      "\n",
      "\n",
      "*/30 06-23 * * * export DISPLAY=:0 && /home/stefano/DATASET_HW2/script.sh\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/fetching_firefox.sh\n",
    "!echo \"\"\n",
    "!cat scripts/crontab_entries_firefox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205f789",
   "metadata": {},
   "source": [
    "To reduce the size of the uploaded zip, I do not include the raw CSVs. They can be downloaded here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV CURL - train\n",
    "!wget \"https://polimi365-my.sharepoint.com/:u:/g/personal/10526141_polimi_it/ESh0NZOxC0dIpDwOWonGPDEB2kKhmdznvfuSADRS7_kdxA?download=1\" -O \"input/CSV_curl.zip\"\n",
    "!unzip \"input/CSV_curl.zip\" -d \"input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV FIREFOX - train\n",
    "!wget \"https://polimi365-my.sharepoint.com/:u:/g/personal/10526141_polimi_it/EWis4276qyJDqaTRBAAlhTcB0gA0k1HFDV25gnbM3syAWg?download=1\" -O \"input/CSV_firefox.zip\"\n",
    "!unzip \"input/CSV_firefox.zip\" -d \"input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af22ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV CURL - some days later\n",
    "!wget \"https://polimi365-my.sharepoint.com/:u:/g/personal/10526141_polimi_it/EcstXWzLgIlPokDeauhr8-0BHjgNxKav7jzx0oDqiA6f-Q?download=1\" -O \"TEST/curl.zip\"\n",
    "!unzip \"TEST/curl.zip\" -d \"TEST/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV FIREFOX - some days later\n",
    "!wget \"https://polimi365-my.sharepoint.com/:u:/g/personal/10526141_polimi_it/Ed0xg5e-IN9PmcbDB7tkPWoBq24hpAwaD1eEuraBstraGw?download=1\" -O \"TEST/firefox.zip\"\n",
    "!unzip \"TEST/firefox.zip\" -d \"TEST/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bb4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, I have 8925 Curl captures and 2277 Firefox captures\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall, I have \" + str(len([name for name in os.listdir('input/CSV_curl')])) + \" Curl captures\"\n",
    "     ' and ' + str(len([name for name in os.listdir('input/CSV_firefox')])) + \" Firefox captures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e67a66",
   "metadata": {},
   "source": [
    "To avoid data leakage and the test influencing in any way the training, I first split into training and testing and then apply normalization on the training data.\n",
    "The obtained scaler values (mean and variance) are then applied to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b8b14",
   "metadata": {},
   "source": [
    "For final testing (1 day later, 3 days later, 7 days later - only for curl), I increase the fetching interval to reduce a bit the number of samples.\n",
    "Notice that I start using Firefox some days after using curl, so the dates are different between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e9add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 day later:  I have 540 Curl captures\n",
      "3 days later: I have 505 Curl captures\n",
      "7 days later: I have 468 Curl captures\n",
      "\n",
      "1 day later:  I have 523 Firefox captures\n",
      "3 days later: I have 522 Firefox captures\n"
     ]
    }
   ],
   "source": [
    "print(\"1 day later:  I have \" + str(len([name for name in os.listdir('TEST/curl/1DAY')])) + \" Curl captures\")\n",
    "print(\"3 days later: I have \" + str(len([name for name in os.listdir('TEST/curl/3DAYS')])) + \" Curl captures\")\n",
    "print(\"7 days later: I have \" + str(len([name for name in os.listdir('TEST/curl/7DAYS')])) + \" Curl captures\")\n",
    "print(\"\")\n",
    "print(\"1 day later:  I have \" + str(len([name for name in os.listdir('TEST/firefox/1DAY')])) + \" Firefox captures\")\n",
    "print(\"3 days later: I have \" + str(len([name for name in os.listdir('TEST/firefox/3DAYS')])) + \" Firefox captures\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
