{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3 - Network Measurement and Data Analysis Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stefano Maxenti, 10526141, 970133*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgRb4mQBd2UY"
   },
   "source": [
    "#**Homework**\n",
    "\n",
    "Complete the following tasks:\n",
    "\n",
    "* Use a dataset of 21 Video Sessions\n",
    "* Recognize the Video Server(s) IP and select video traffic (***if more than one Server is found, keep the dominant flow only***)\n",
    "* Detect Video Client HTTP Requests (Uplink packets with size larger or equal to 100 Bytes)\n",
    "* Compute features to predict:\n",
    " 1.   When the next UL Request is sent by the Video Client \n",
    " 2.   How large is the response of the Server to the next UL Request\n",
    "\n",
    "**N.B.**: Below, you can find a list of useful functions for the tasks at hand (introduced during class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "[LIBRARIES AND FUNCTIONS](#libraries_and_functions)\n",
    "\n",
    "[CLASS APPROACH](#class)\n",
    "\n",
    "[CACHED APPROACH](#cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and functions\n",
    "<a id='libraries_and_functions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zJxa-o59dwkB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/5967539\n",
    "# Very basic implementation of human ordering, useful to read all files in order.\n",
    "# It will prove useful when we preprocess data because order might have an effect.\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DsJgewH_DVk"
   },
   "source": [
    "### Functions Ready-To-Use (although with some modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_traffic(data, domain, cached_ips, opt=False): # the opt parameter will be explained later on\n",
    "   \n",
    "    # Look in DNS Responses for googlevideo domain\n",
    "    dns_data = data[data['Protocol']=='DNS']\n",
    "    dns = dns_data[dns_data['Info'].apply(lambda x: 'googlevideo' in x and 'response' in x)]\n",
    "    ips = dns.Address.values\n",
    "    if (opt):\n",
    "        for i in ips:\n",
    "            cached_ips.append(i)\n",
    "        ips = cached_ips\n",
    "\n",
    "    server_names = dns.Name.values\n",
    "\n",
    "    # Filtering on either \"Source\" or \"Destination\" IP, get the \n",
    "    # rows of the dataset that contain at least one of the selected IPs\n",
    "    downlink = data[data['Source'].apply(lambda x: x in ips)].dropna(subset=['Length']) \n",
    "\n",
    "    uplink = data[data['Destination'].apply(lambda x: x in ips)].dropna(subset=['Length'])\n",
    "    \n",
    "    return uplink, downlink, ips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-EKk7Xqfd6gI"
   },
   "outputs": [],
   "source": [
    "def find_dominant(uplink, downlink, verbose=False):\n",
    "\n",
    "    # Expressed in MB\n",
    "\n",
    "    # Order flows by cumulative DL Volume\n",
    "    flows_DL = downlink.groupby(['Source','Destination'])['Length'].sum()/(10**6)\n",
    "    # Get (Source,Destination) IPs of dominant flows\n",
    "    dom_id = flows_DL[flows_DL==max(flows_DL)].index[0]\n",
    "    if (verbose):\n",
    "        print(flows_DL)\n",
    "        print(\"DOM_ID:\", dom_id)\n",
    "    # Filter traffic selecting the dominant flow\n",
    "    dom_dl = downlink[downlink['Source'] == dom_id[0]]\n",
    "    dom_ul = uplink[(uplink['Source'] == dom_id[1])]\n",
    "    return dom_ul, dom_dl\n",
    "\n",
    "\n",
    "def timebased_filter(data, length=None, min_time=None, max_time=None):\n",
    "    '''\n",
    "    :param data: pd dataframe to be filtered. Must contain columns: \"Length\" and \"Time\"\n",
    "    :param length: all packets shorter than length [Bytes] will be discarded (default 0)\n",
    "    :param min_time: all packets with timestamp smaller than min_time [s] will be discarded (default 0)\n",
    "    :param max_time: all packets with timestamp larger than max_time [s] will be discarded (default 1000)\n",
    "    '''\n",
    "\n",
    "    if length is None:\n",
    "        length=0\n",
    "    if min_time is None:\n",
    "        min_time = 0\n",
    "    if max_time is None:\n",
    "        max_time = 1000\n",
    "  \n",
    "    filtered_data = data.copy().reset_index()\n",
    "    mask = (filtered_data['Length']>=length) & (filtered_data['Time']>=min_time) & (filtered_data['Time']<= max_time)\n",
    "    filtered_data = filtered_data.loc[mask[mask ==True].index]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def find_next(array, value):\n",
    "    '''\n",
    "    :param array: np.array, array of floats\n",
    "    :param value: float, reference value\n",
    "    :return: position of the closest element of the array greater than \"value\"\n",
    "    '''\n",
    "    delta = np.asarray(array) - value\n",
    "    idx = np.where(delta >= 0, delta, np.inf).argmin()\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic standard normalization\n",
    "def normalize_dataset(training_set, test_set):\n",
    "    \n",
    "    mean_train = training_set.mean()\n",
    "    std_train = training_set.std()\n",
    "    norm_train = (training_set - mean_train)/std_train\n",
    "    norm_test = (test_set - mean_train)/std_train  \n",
    "\n",
    "    return norm_train, norm_test, mean_train, std_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMO_nFab_IPu"
   },
   "source": [
    "### Functions to be completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extraction(uplink, downlink, playback_start=2, playback_end=180, min_ul_size=100, min_dl_size=50):\n",
    "    '''\n",
    "    Complete this function to extract both features and groundtruth.\n",
    "\n",
    "    NB: The features extraction process is the same as the one introduced during\n",
    "    the lecture. \n",
    "    '''\n",
    "    \n",
    "    uplink = timebased_filter(uplink, min_ul_size, playback_start, playback_end)\n",
    "    downlink = timebased_filter(downlink, min_dl_size, playback_start, playback_end)\n",
    "\n",
    "    dataset = pd.DataFrame(columns=['Request_Size','Inter_RR_Time','DL_Time','DL_Vol','DL_Size','PB_Time'])\n",
    "    # ****************************************************************************\n",
    "    # Feature 1: Client Request Size\n",
    "    dataset['Request_Size'] = list(uplink.Length.values)\n",
    "\n",
    "    # ****************************************************************************\n",
    "    # Feature 2: Inter Request-Response Time\n",
    "    rr_time = []\n",
    "    response_time = []\n",
    "    for t in uplink.Time:\n",
    "        response_time.append(find_next(downlink.Time, t)) #index of next DL packet timestamp \n",
    "        rr_time.append(downlink.Time.iloc[response_time[-1]] - t)\n",
    "\n",
    "    dataset['Inter_RR_Time'] = rr_time\n",
    "\n",
    "    # ****************************************************************************\n",
    "    # Feature 3-4-5: Download Time, Download Volume, Download Size (# Packets) \n",
    "    dt = []\n",
    "    dv = []\n",
    "    ds = []\n",
    "\n",
    "    for rt1, rt2 in zip(response_time[:-1], response_time[1:]):\n",
    "        \n",
    "        #Download Time\n",
    "        dt.append(downlink.Time.iloc[rt2-1] - downlink.Time.iloc[rt1])\n",
    "\n",
    "        temp = timebased_filter(downlink, 0, downlink.Time.iloc[rt1], downlink.Time.iloc[rt2-1])\n",
    "\n",
    "        #Download Volume\n",
    "        dv.append(temp.Length.sum())\n",
    "\n",
    "        #Download Size (# Packets) \n",
    "        ds.append(temp.shape[0])\n",
    "\n",
    "    # Last Iteration data might be corrupted due to drastic interruption of capture \n",
    "    # process. If it is so, an error would occur during the features extraction.\n",
    "    # To avoid this, we skip last HTTP iteration data when an error is raised \n",
    "    # using the try-except logic below.\n",
    "    try:\n",
    "        # Consider also last HTTP iteration\n",
    "        #Download Time\n",
    "        dt.append(downlink.Time.iloc[-1] - downlink.Time.iloc[rt2])\n",
    "\n",
    "        temp = timebased_filter(downlink, 0, downlink.Time.iloc[rt2], downlink.Time.iloc[-1])\n",
    "        #Download Volume\n",
    "        dv.append(temp.Length.sum())\n",
    "\n",
    "        #Download Size (# Packets) \n",
    "        ds.append(temp.shape[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    dataset['DL_Time'] = dt\n",
    "    dataset['DL_Vol'] = dv\n",
    "    dataset['DL_Size'] = ds\n",
    "\n",
    "    # ****************************************************************************\n",
    "    # Feature 5: Playback Time\n",
    "    pbt = list(uplink.Time.values)\n",
    "    dataset['PB_Time'] = pbt\n",
    "    # ****************************************************************************\n",
    "\n",
    "    #print(dataset)\n",
    "    # Check Features Consistency\n",
    "    dataset = dataset[(dataset > 0).all(1)]\n",
    "    dataset = dataset[dataset['DL_Time']<20]\n",
    "    #print(len(dataset))\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # TO BE COMPLETED\n",
    "    ### EXTRACT GROUNDTRUTH HERE\n",
    "    groundtruth = pd.DataFrame(columns=['Next_Request_Time','Next_Response_Vol'])\n",
    "    # ****************************************************************************\n",
    "    # GT 1: Next Request Time\n",
    "    indexes = dataset.index\n",
    "    service_time_list = []\n",
    "    for x in indexes: # we iterate on all indexes\n",
    "        service_time = dataset.loc[x]['Inter_RR_Time'] + dataset.loc[x]['DL_Time'] # this is the service time of the packet\n",
    "        service_time_list.append(service_time)\n",
    "    # The next request time happens after the difference between two consecutive playback times,\n",
    "    # but we also need to remove the service time of the last packet.\n",
    "    #print(len(service_time_list))\n",
    "    groundtruth['Next_Request_Time'] = dataset['PB_Time'].diff().shift(periods=-1) - service_time_list\n",
    "    # ****************************************************************************\n",
    "    # GT 2: Next Response Volume\n",
    "    #print(groundtruth)\n",
    "  \n",
    "    groundtruth['Next_Response_Vol'] = dataset['DL_Vol'].shift(periods=-1) # nothing to do, we just have to copy the column\n",
    "    ## In both the features, shift is needed to preserve the indexes, otherwise they would be messed up.\n",
    "    ###############################################################\n",
    "\n",
    "    groundtruth.dropna(inplace=True)\n",
    "    \n",
    "    intersection = set(dataset.index).intersection(set(groundtruth.index))\n",
    "    dataset = dataset.loc[intersection,:]\n",
    "    groundtruth = groundtruth.loc[intersection,:]\n",
    "\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a general mock to train and test various regressors, while performing K-Fold-Cross-Validation and applying normalization on both train and test.\n",
    "\n",
    "I want to compare the performance of various regressors, using the abstractness allowed by sklearn.\n",
    "I use:\n",
    "* Random forest regressor\n",
    "* Multi-Layer-Perceptron regressor\n",
    "* Ridge regressor\n",
    "* Lasso regressor\n",
    "* ElasticNet regressor\n",
    "* ExtraTrees regressor\n",
    "* DecisionTrees regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_list = [RandomForestRegressor(), MLPRegressor(), Ridge(), Lasso(), ElasticNet(), ExtraTreesRegressor(), DecisionTreeRegressor()]\n",
    "\n",
    "def train_test_model(model, kf, X, y):\n",
    "    rmse_request_time = []\n",
    "    rmse_response_vol = []\n",
    "    for train, test in kf.split(X, y):\n",
    "        X_train, X_test, mean0, std0  = normalize_dataset(X.iloc[train], X.iloc[test])\n",
    "        y_train, y_test, mean1, std1  = normalize_dataset(y.iloc[train], y.iloc[test])\n",
    "        #y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "        \n",
    "        # Just a note on the following lines: differently from keras, each time we call\n",
    "        # fit on a model, it retrains from stratch, not from the state it was before.\n",
    "        # See: https://scikit-learn.org/stable/tutorial/basic/tutorial.html#refitting-and-updating-parameters\n",
    "        # So it is like we are explicitly creating a new model for each fold, \n",
    "        # which is the way to go when performing K-Fold-Cross-Validation.\n",
    "        # Maybe if would be better for readability to explicitly re-create the model as in the comment below..\n",
    "        # This breaks extendibility of the code, though.\n",
    "        '''\n",
    "        if (model.__class__.__name__ is \"RandomForest\"):\n",
    "            model = RandomForestRegressor()\n",
    "        if (model.__class__.__name__ is \"MLP\"):\n",
    "            model = MLPRegressor()\n",
    "        if (model.__class__.__name__ is \"Ridge\"):\n",
    "            model = Ridge()\n",
    "        if (model.__class__.__name__ is \"Lasso\"):\n",
    "            model = Lasso()\n",
    "        if (model.__class__.__name__ is \"ElasticNet\"):\n",
    "            model = ElasticNet()\n",
    "        if (model.__class__.__name__ is \"ExtraTreesRegressor\"):\n",
    "            model = ExtraTreesRegressor()\n",
    "        if (model.__class__.__name__ is \"DecisionTreeRegressor\"):\n",
    "            model = DecisionTreeRegressor()\n",
    "        '''\n",
    "        model.fit(X_train, y_train)\n",
    "        p = model.predict(X_test) * std1.values + mean1.values # denormalization before computing RMSEs\n",
    "        rmse_rt_ = math.sqrt(metrics.mean_squared_error((y.iloc[test])['Next_Request_Time'], pd.DataFrame(p)[0]))\n",
    "        rmse_rv_ = math.sqrt(metrics.mean_squared_error((y.iloc[test])['Next_Response_Vol'], pd.DataFrame(p)[1]))/1000 # to KB\n",
    "        rmse_request_time.append(rmse_rt_)\n",
    "        rmse_response_vol.append(rmse_rv_)\n",
    "    return rmse_request_time, rmse_response_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d45P-M-v_PKm"
   },
   "source": [
    "## Class approach\n",
    "<a id='class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply process all CSV files in the path and returns the dataset and the groundtruth as two different Dataframe with aligned indexes.\n",
    "\n",
    "The *opt* parameter will be explained later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(path, opt=False, verbose=False, save=False, remove_outliers=True):\n",
    "    tcpdumpfiles = [f for f in listdir(path) if (isfile(join(path, f)) and splitext(join(path,f))[-1] == '.csv')]\n",
    "    tcpdumpfiles.sort(key=natural_keys)\n",
    "\n",
    "    X = pd.DataFrame() # dataset\n",
    "    y = pd.DataFrame() # groundtruth\n",
    "\n",
    "    cached_ips = [] # explained later on\n",
    "    for f in tcpdumpfiles:\n",
    "        df = pd.read_csv(path+\"/\"+f)\n",
    "        print(f, end = \": \")\n",
    "        domain_name = 'googlevideo'\n",
    "        uplink, downlink, cached_ips = filter_traffic(df, domain_name, cached_ips, opt=opt)\n",
    "        dom_ul, dom_dl = find_dominant(uplink, downlink, verbose=verbose)\n",
    "        \n",
    "        if (opt):\n",
    "            cached_ips = np.unique(cached_ips).tolist()\n",
    "            if (verbose):\n",
    "                print(\"cached ips: \" ,cached_ips)\n",
    "                print(\"\\n\\n\")\n",
    "            \n",
    "        X_, y_ = (features_extraction(dom_ul, dom_dl))\n",
    "        \n",
    "        print(\"DS:\", len(X_), \"GT:\", len(y_)) # these values must be the same\n",
    "\n",
    "        # apparently pandas.append() is deprecated\n",
    "        X = pd.concat([X, X_], ignore_index=True)\n",
    "        y = pd.concat([y, y_], ignore_index=True)\n",
    "    if (remove_outliers): # we use Z-Score metric to filter out outliers\n",
    "        df = pd.concat([X, y], axis=1) # we need to merge all features\n",
    "        df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "        X = df[['Request_Size', 'Inter_RR_Time', 'DL_Time', 'DL_Vol', 'DL_Size','PB_Time']]\n",
    "        y = df[['Next_Request_Time', 'Next_Response_Vol']]\n",
    "        \n",
    "    if (save): # to save the resuling CSVs with different names according to the parameters\n",
    "        if (opt):\n",
    "            X.to_csv('dataset_opt.csv', index=False)\n",
    "            y.to_csv('groundtruth_opt.csv', index=False)\n",
    "        else:\n",
    "            X.to_csv('dataset.csv', index=False)\n",
    "            y.to_csv('groundtruth.csv', index=False)\n",
    "    print(\"\\n\\n\\nX:\" ,len(X), \"y: \" ,len(y)) # these values must be the same\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture_v2_0.csv: DS: 46 GT: 46\n",
      "Capture_v2_1.csv: DS: 42 GT: 42\n",
      "Capture_v2_2.csv: DS: 38 GT: 38\n",
      "Capture_v2_3.csv: DS: 0 GT: 0\n",
      "Capture_v2_4.csv: DS: 24 GT: 24\n",
      "Capture_v2_5.csv: DS: 9 GT: 9\n",
      "Capture_v2_6.csv: DS: 6 GT: 6\n",
      "Capture_v2_7.csv: DS: 76 GT: 76\n",
      "Capture_v2_8.csv: DS: 18 GT: 18\n",
      "Capture_v2_9.csv: DS: 6 GT: 6\n",
      "Capture_v2_10.csv: DS: 4 GT: 4\n",
      "Capture_v2_11.csv: DS: 4 GT: 4\n",
      "Capture_v2_12.csv: DS: 8 GT: 8\n",
      "Capture_v2_13.csv: DS: 5 GT: 5\n",
      "Capture_v2_14.csv: DS: 12 GT: 12\n",
      "Capture_v2_15.csv: DS: 1 GT: 1\n",
      "Capture_v2_16.csv: DS: 15 GT: 15\n",
      "Capture_v2_17.csv: DS: 6 GT: 6\n",
      "Capture_v2_18.csv: DS: 7 GT: 7\n",
      "Capture_v2_19.csv: DS: 1 GT: 1\n",
      "Capture_v2_20.csv: DS: 10 GT: 10\n",
      "Capture_v2_21.csv: DS: 6 GT: 6\n",
      "\n",
      "\n",
      "\n",
      "X: 318 y:  318\n"
     ]
    }
   ],
   "source": [
    "path = 'Captures'\n",
    "X, y = preprocess_data(path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use cross validation to have an idea on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS\n",
    "X = pd.read_csv('dataset.csv')\n",
    "y = pd.read_csv('groundtruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RandomForestRegressor \n",
      "\t\t\t\t 4.366923345995478 359.83011705101086\n",
      "***********************************************************************************\n",
      "\n",
      " MLPRegressor \n",
      "\t\t\t\t 6.778015256899018 506.0815759404145\n",
      "***********************************************************************************\n",
      "\n",
      " Ridge \n",
      "\t\t\t\t 4.731863523052492 444.2557409898737\n",
      "***********************************************************************************\n",
      "\n",
      " Lasso \n",
      "\t\t\t\t 4.7181510966379285 484.95480563411684\n",
      "***********************************************************************************\n",
      "\n",
      " ElasticNet \n",
      "\t\t\t\t 4.7181510966379285 445.23858414871285\n",
      "***********************************************************************************\n",
      "\n",
      " ExtraTreesRegressor \n",
      "\t\t\t\t 4.51564053636824 368.4599840725476\n",
      "***********************************************************************************\n",
      "\n",
      " DecisionTreeRegressor \n",
      "\t\t\t\t 5.874781336508752 481.90549377897185\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "for reg in regressor_list:\n",
    "    rmse_request_time, rmse_response_vol = train_test_model(reg, kf, X, y)\n",
    "    print(\"\\n\", reg.__class__.__name__, \"\\n\\t\\t\\t\\t\", statistics.mean(rmse_request_time), statistics.mean(rmse_response_vol))\n",
    "    print(\"***********************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next_Request_Time* is predicted almost equally in all regressors but MLP, whereas *Next_Response_Vol* varies more.\n",
    "\n",
    "The best performance for both features is obtained with a Random Forest regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached approach (a possible improvement)\n",
    "<a id='cached'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, I have just proceeded following the same approach used in class. I notice something strange though: a good number of captures shows 0 (or very low) sequences.\n",
    "\n",
    "In addition to that, here I show the size of the dominant flow per file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture_v2_0.csv 7.199843 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    48.812378\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_1.csv 7.598801 MB\n",
      "Source         Destination\n",
      "91.81.217.141  192.168.1.6    9.993622\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_2.csv 8.549215 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    44.867013\n",
      "91.81.217.141  192.168.1.6    13.785031\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_3.csv 8.851757 MB\n",
      "Source          Destination\n",
      "74.125.163.138  192.168.1.6    0.008266\n",
      "74.125.99.91    192.168.1.6    0.008056\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.163.138', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_4.csv 8.607993 MB\n",
      "Source          Destination\n",
      "74.125.104.103  192.168.1.6     0.012088\n",
      "74.125.111.106  192.168.1.6     0.015628\n",
      "74.125.154.138  192.168.1.6     1.054021\n",
      "91.81.217.140   192.168.1.6    17.478906\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_5.csv 8.760167 MB\n",
      "Source          Destination\n",
      "74.125.111.105  192.168.1.6    7.992543\n",
      "74.125.99.105   192.168.1.6    0.009947\n",
      "91.81.217.140   192.168.1.6    2.965944\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.111.105', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_6.csv 8.851672 MB\n",
      "Source         Destination\n",
      "74.125.153.24  192.168.1.6    4.127591\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.153.24', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_7.csv 8.632314 MB\n",
      "Source          Destination\n",
      "74.125.111.106  192.168.1.6     2.080363\n",
      "91.81.217.141   192.168.1.6    32.572815\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_8.csv 8.451419 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6    0.839776\n",
      "173.194.187.71   192.168.1.6    1.781121\n",
      "74.125.105.10    192.168.1.6    0.012088\n",
      "91.81.217.140    192.168.1.6    8.902818\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_9.csv 8.047804 MB\n",
      "Source           Destination\n",
      "173.194.188.105  192.168.1.6    1.686488\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('173.194.188.105', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_10.csv 8.155929 MB\n",
      "Source           Destination\n",
      "173.194.160.219  192.168.1.6    0.008240\n",
      "173.194.188.72   192.168.1.6    1.974872\n",
      "74.125.153.59    192.168.1.6    0.012162\n",
      "74.125.99.108    192.168.1.6    0.015762\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('173.194.188.72', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_11.csv 7.896718 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    2.106974\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_12.csv 8.10944 MB\n",
      "Source         Destination\n",
      "209.85.226.38  192.168.1.6    0.549426\n",
      "74.125.162.39  192.168.1.6    0.015674\n",
      "74.125.162.40  192.168.1.6    2.380683\n",
      "74.125.99.170  192.168.1.6    0.015766\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.162.40', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_13.csv 7.929232 MB\n",
      "Source         Destination\n",
      "91.81.217.141  192.168.1.6    0.359344\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_14.csv 7.893808 MB\n",
      "Source         Destination\n",
      "74.125.99.169  192.168.1.6    0.602666\n",
      "91.81.217.140  192.168.1.6    3.844219\n",
      "91.81.217.141  192.168.1.6    0.012511\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_15.csv 7.967527 MB\n",
      "Source           Destination\n",
      "172.217.132.137  192.168.1.6    0.414788\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('172.217.132.137', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_16.csv 7.504994 MB\n",
      "Source           Destination\n",
      "173.194.182.138  192.168.1.6    0.015734\n",
      "173.194.187.136  192.168.1.6    1.495762\n",
      "74.125.110.102   192.168.1.6    0.866399\n",
      "74.125.160.202   192.168.1.6    2.283527\n",
      "74.125.4.230     192.168.1.6    0.005583\n",
      "74.125.99.106    192.168.1.6    1.023713\n",
      "74.125.99.72     192.168.1.6    1.731761\n",
      "91.81.217.140    192.168.1.6    6.745528\n",
      "91.81.217.141    192.168.1.6    0.021178\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_17.csv 7.095274 MB\n",
      "Source           Destination\n",
      "173.194.188.136  192.168.1.6    0.158705\n",
      "74.125.111.102   192.168.1.6    1.924227\n",
      "74.125.153.11    192.168.1.6    0.008252\n",
      "74.125.99.166    192.168.1.6    1.296750\n",
      "74.125.99.168    192.168.1.6    0.652067\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.111.102', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_18.csv 7.475286 MB\n",
      "Source         Destination\n",
      "74.125.99.137  192.168.1.6    1.441036\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.99.137', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_19.csv 7.646476 MB\n",
      "Source           Destination\n",
      "173.194.182.135  192.168.1.6    0.017631\n",
      "74.125.99.168    192.168.1.6    0.199115\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.99.168', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_20.csv 6.958159 MB\n",
      "Source           Destination\n",
      "173.194.182.230  192.168.1.6    1.706543\n",
      "173.194.188.230  192.168.1.6    0.853296\n",
      "74.125.153.7     192.168.1.6    1.060220\n",
      "74.125.99.108    192.168.1.6    1.938626\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.99.108', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_21.csv 7.041897 MB\n",
      "Source         Destination\n",
      "74.125.160.38  192.168.1.6    0.008204\n",
      "74.125.99.168  192.168.1.6    0.410873\n",
      "74.125.99.170  192.168.1.6    2.693258\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.99.170', '192.168.1.6')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Captures'\n",
    "tcpdumpfiles = [f for f in listdir(path) if (isfile(join(path, f)) and splitext(join(path,f))[-1] == '.csv')]\n",
    "tcpdumpfiles.sort(key=natural_keys)\n",
    "opt = False\n",
    "verbose = True\n",
    "\n",
    "dummy = [] # not needed here, just a placeholder\n",
    "\n",
    "for f in tcpdumpfiles:\n",
    "    df = pd.read_csv(path+\"/\"+f)\n",
    "    print(f, os.path.getsize(path+'/'+f)/10**6, \"MB\")\n",
    "    \n",
    "    domain_name = 'googlevideo'\n",
    "    uplink, downlink, dummy = filter_traffic(df,domain_name, dummy, opt=opt)\n",
    "    dom_ul, dom_dl = find_dominant(uplink, downlink, verbose=verbose)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the dominant flow from some captures (for example, \"Capture_v2_3.csv\") is less than 1 MB long. A 3-minute long YouTube video tends to be bigger. The capture is however quite big.\n",
    "\n",
    "My first hyphothesis is that there is some extra traffic, maybe coming from other applications.\n",
    "\n",
    "To confirm or deny it, I need some more information.\n",
    "\n",
    "The *opt* parameter comes in handy: it is used not only to look for *googlevideo* domain in the new capture, but to keep in memory as well all previous IPs belonging to YouTube in a list (*cached_ips*). In this way, we look for all *googlevideo* domains but we also check whether there is a IP used in a previous capture to provide YouTube content. Using this list avoids to get flows from different applications than the one required by the homework.\n",
    "\n",
    "I then call the *find_dominant()* function with the verbose flag to see all flows in the capture.\n",
    "\n",
    "Let's find out what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture_v2_0.csv 7.199843 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    48.812378\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_1.csv 7.598801 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    42.213374\n",
      "91.81.217.141  192.168.1.6     9.993622\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_2.csv 8.549215 MB\n",
      "Source         Destination\n",
      "91.81.217.140  192.168.1.6    44.867013\n",
      "91.81.217.141  192.168.1.6    13.785031\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_3.csv 8.851757 MB\n",
      "Source          Destination\n",
      "74.125.163.138  192.168.1.6     0.008266\n",
      "74.125.99.91    192.168.1.6     0.008056\n",
      "91.81.217.140   192.168.1.6    22.007896\n",
      "91.81.217.141   192.168.1.6    26.819344\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_4.csv 8.607993 MB\n",
      "Source          Destination\n",
      "74.125.104.103  192.168.1.6     0.012088\n",
      "74.125.111.106  192.168.1.6     0.015628\n",
      "74.125.154.138  192.168.1.6     1.054021\n",
      "91.81.217.140   192.168.1.6    17.478906\n",
      "91.81.217.141   192.168.1.6    29.844017\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_5.csv 8.760167 MB\n",
      "Source          Destination\n",
      "74.125.111.105  192.168.1.6     7.992543\n",
      "74.125.111.106  192.168.1.6     0.000132\n",
      "74.125.154.138  192.168.1.6     4.578662\n",
      "74.125.99.105   192.168.1.6     0.009947\n",
      "91.81.217.140   192.168.1.6     2.965944\n",
      "91.81.217.141   192.168.1.6    31.034067\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_6.csv 8.851672 MB\n",
      "Source          Destination\n",
      "74.125.111.105  192.168.1.6     8.643315\n",
      "74.125.153.24   192.168.1.6     4.127591\n",
      "74.125.154.138  192.168.1.6     3.009834\n",
      "91.81.217.140   192.168.1.6     1.919914\n",
      "91.81.217.141   192.168.1.6    31.672533\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_7.csv 8.632314 MB\n",
      "Source          Destination\n",
      "74.125.111.105  192.168.1.6     5.878677\n",
      "74.125.111.106  192.168.1.6     2.080363\n",
      "74.125.153.24   192.168.1.6     7.626226\n",
      "74.125.154.138  192.168.1.6     0.439054\n",
      "91.81.217.140   192.168.1.6     0.673832\n",
      "91.81.217.141   192.168.1.6    32.572815\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_8.csv 8.451419 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     0.839776\n",
      "173.194.187.71   192.168.1.6     1.781121\n",
      "74.125.105.10    192.168.1.6     0.012088\n",
      "74.125.111.105   192.168.1.6     4.719979\n",
      "74.125.111.106   192.168.1.6    16.848337\n",
      "74.125.153.24    192.168.1.6     3.921984\n",
      "74.125.154.138   192.168.1.6     3.315906\n",
      "91.81.217.140    192.168.1.6     8.902818\n",
      "91.81.217.141    192.168.1.6     7.183747\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.111.106', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_9.csv 8.047804 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     1.909531\n",
      "173.194.187.71   192.168.1.6     0.991789\n",
      "173.194.188.105  192.168.1.6     1.686488\n",
      "74.125.111.105   192.168.1.6     2.421693\n",
      "74.125.111.106   192.168.1.6     5.027232\n",
      "74.125.153.24    192.168.1.6     0.000132\n",
      "74.125.154.138   192.168.1.6     4.696914\n",
      "91.81.217.140    192.168.1.6    13.911117\n",
      "91.81.217.141    192.168.1.6    10.554448\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_10.csv 8.155929 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     1.069992\n",
      "173.194.160.219  192.168.1.6     0.008240\n",
      "173.194.187.71   192.168.1.6     0.869614\n",
      "173.194.188.105  192.168.1.6     8.205070\n",
      "173.194.188.72   192.168.1.6     1.974872\n",
      "74.125.111.105   192.168.1.6     0.000132\n",
      "74.125.153.59    192.168.1.6     0.012162\n",
      "74.125.154.138   192.168.1.6     3.394864\n",
      "74.125.99.108    192.168.1.6     0.015762\n",
      "91.81.217.140    192.168.1.6     5.470630\n",
      "91.81.217.141    192.168.1.6    22.118002\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_11.csv 7.896718 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     3.188012\n",
      "173.194.187.71   192.168.1.6     2.059975\n",
      "173.194.188.105  192.168.1.6     2.907665\n",
      "173.194.188.72   192.168.1.6     1.702779\n",
      "74.125.154.138   192.168.1.6     1.471856\n",
      "74.125.99.108    192.168.1.6     0.000396\n",
      "91.81.217.140    192.168.1.6     2.106974\n",
      "91.81.217.141    192.168.1.6    26.754049\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_12.csv 8.10944 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     1.162063\n",
      "173.194.187.71   192.168.1.6     1.912040\n",
      "173.194.188.105  192.168.1.6     8.200918\n",
      "173.194.188.72   192.168.1.6     2.223294\n",
      "209.85.226.38    192.168.1.6     0.549426\n",
      "74.125.154.138   192.168.1.6     3.379687\n",
      "74.125.162.39    192.168.1.6     0.015674\n",
      "74.125.162.40    192.168.1.6     2.380683\n",
      "74.125.99.170    192.168.1.6     0.015766\n",
      "91.81.217.140    192.168.1.6     4.637870\n",
      "91.81.217.141    192.168.1.6    19.581330\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_13.csv 7.929232 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6    1.093953\n",
      "173.194.187.71   192.168.1.6    1.241300\n",
      "173.194.188.105  192.168.1.6    3.076482\n",
      "173.194.188.72   192.168.1.6    2.080570\n",
      "209.85.226.38    192.168.1.6    4.682197\n",
      "74.125.154.138   192.168.1.6    9.270649\n",
      "74.125.162.39    192.168.1.6    0.000412\n",
      "74.125.162.40    192.168.1.6    0.000066\n",
      "74.125.99.170    192.168.1.6    0.000396\n",
      "91.81.217.140    192.168.1.6    4.561593\n",
      "91.81.217.141    192.168.1.6    0.359344\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('74.125.154.138', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_14.csv 7.893808 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6    1.471656\n",
      "173.194.187.71   192.168.1.6    1.285929\n",
      "173.194.188.105  192.168.1.6    6.297404\n",
      "173.194.188.72   192.168.1.6    0.790369\n",
      "209.85.226.38    192.168.1.6    3.040978\n",
      "74.125.154.138   192.168.1.6    3.902536\n",
      "74.125.99.169    192.168.1.6    0.602666\n",
      "91.81.217.140    192.168.1.6    3.844219\n",
      "91.81.217.141    192.168.1.6    0.012511\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('173.194.188.105', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_15.csv 7.967527 MB\n",
      "Source           Destination\n",
      "172.217.132.137  192.168.1.6     0.414788\n",
      "173.194.160.200  192.168.1.6     1.525798\n",
      "173.194.187.71   192.168.1.6     0.991416\n",
      "173.194.188.105  192.168.1.6     2.471719\n",
      "173.194.188.72   192.168.1.6     0.843470\n",
      "209.85.226.38    192.168.1.6     2.760226\n",
      "74.125.154.138   192.168.1.6     4.309661\n",
      "74.125.99.169    192.168.1.6     2.709915\n",
      "91.81.217.140    192.168.1.6     1.745773\n",
      "91.81.217.141    192.168.1.6    12.663941\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.141', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_16.csv 7.504994 MB\n",
      "Source           Destination\n",
      "172.217.132.137  192.168.1.6    4.267168\n",
      "173.194.160.200  192.168.1.6    0.506444\n",
      "173.194.182.138  192.168.1.6    0.015734\n",
      "173.194.187.136  192.168.1.6    1.495762\n",
      "173.194.187.71   192.168.1.6    0.513504\n",
      "173.194.188.105  192.168.1.6    0.000066\n",
      "173.194.188.72   192.168.1.6    1.516664\n",
      "209.85.226.38    192.168.1.6    0.989630\n",
      "74.125.110.102   192.168.1.6    0.866399\n",
      "74.125.111.106   192.168.1.6    0.486621\n",
      "74.125.154.138   192.168.1.6    2.546170\n",
      "74.125.160.202   192.168.1.6    2.283527\n",
      "74.125.4.230     192.168.1.6    0.005583\n",
      "74.125.99.106    192.168.1.6    1.023713\n",
      "74.125.99.72     192.168.1.6    1.731761\n",
      "91.81.217.140    192.168.1.6    6.745528\n",
      "91.81.217.141    192.168.1.6    0.021178\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_17.csv 7.095274 MB\n",
      "Source           Destination\n",
      "172.217.132.137  192.168.1.6     0.000132\n",
      "173.194.160.200  192.168.1.6     2.094672\n",
      "173.194.182.138  192.168.1.6     0.000396\n",
      "173.194.187.136  192.168.1.6     0.000132\n",
      "173.194.187.71   192.168.1.6     0.411652\n",
      "173.194.188.136  192.168.1.6     0.158705\n",
      "173.194.188.72   192.168.1.6     0.503484\n",
      "74.125.110.102   192.168.1.6     0.000132\n",
      "74.125.111.102   192.168.1.6     1.924227\n",
      "74.125.111.106   192.168.1.6     0.683594\n",
      "74.125.153.11    192.168.1.6     0.008252\n",
      "74.125.154.138   192.168.1.6     1.826884\n",
      "74.125.160.202   192.168.1.6     3.109530\n",
      "74.125.99.105    192.168.1.6     0.000264\n",
      "74.125.99.106    192.168.1.6     0.000132\n",
      "74.125.99.166    192.168.1.6     1.296750\n",
      "74.125.99.168    192.168.1.6     0.652067\n",
      "74.125.99.72     192.168.1.6     0.000066\n",
      "91.81.217.140    192.168.1.6    12.590135\n",
      "91.81.217.141    192.168.1.6     0.009265\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture_v2_18.csv 7.475286 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     1.708232\n",
      "173.194.187.71   192.168.1.6     0.335498\n",
      "173.194.188.136  192.168.1.6     0.187882\n",
      "173.194.188.72   192.168.1.6     1.005700\n",
      "74.125.111.102   192.168.1.6     2.673684\n",
      "74.125.111.106   192.168.1.6     0.984788\n",
      "74.125.154.138   192.168.1.6     1.947016\n",
      "74.125.160.202   192.168.1.6     1.663054\n",
      "74.125.99.137    192.168.1.6     1.441036\n",
      "74.125.99.166    192.168.1.6     0.200582\n",
      "74.125.99.168    192.168.1.6     0.493608\n",
      "91.81.217.140    192.168.1.6    15.990186\n",
      "91.81.217.141    192.168.1.6     0.004896\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_19.csv 7.646476 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     0.659220\n",
      "173.194.182.135  192.168.1.6     0.017631\n",
      "173.194.187.71   192.168.1.6     0.329843\n",
      "173.194.188.136  192.168.1.6     0.000264\n",
      "173.194.188.72   192.168.1.6     1.513027\n",
      "74.125.111.102   192.168.1.6     1.172326\n",
      "74.125.111.106   192.168.1.6     0.511507\n",
      "74.125.154.138   192.168.1.6     1.611467\n",
      "74.125.160.202   192.168.1.6     1.725782\n",
      "74.125.99.137    192.168.1.6     1.051638\n",
      "74.125.99.168    192.168.1.6     0.199115\n",
      "91.81.217.140    192.168.1.6    14.137038\n",
      "91.81.217.141    192.168.1.6    12.780957\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_20.csv 6.958159 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6    1.520492\n",
      "173.194.182.230  192.168.1.6    1.706543\n",
      "173.194.187.71   192.168.1.6    0.877914\n",
      "173.194.188.136  192.168.1.6    0.482658\n",
      "173.194.188.230  192.168.1.6    0.853296\n",
      "173.194.188.72   192.168.1.6    0.340548\n",
      "74.125.111.102   192.168.1.6    1.850600\n",
      "74.125.111.106   192.168.1.6    0.541936\n",
      "74.125.153.7     192.168.1.6    1.060220\n",
      "74.125.154.138   192.168.1.6    1.395432\n",
      "74.125.160.202   192.168.1.6    1.473116\n",
      "74.125.99.108    192.168.1.6    1.938626\n",
      "74.125.99.137    192.168.1.6    0.000066\n",
      "74.125.99.168    192.168.1.6    0.483533\n",
      "91.81.217.140    192.168.1.6    9.446061\n",
      "91.81.217.141    192.168.1.6    8.248488\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n",
      "Capture_v2_21.csv 7.041897 MB\n",
      "Source           Destination\n",
      "173.194.160.200  192.168.1.6     0.604028\n",
      "173.194.182.135  192.168.1.6     0.469475\n",
      "173.194.182.230  192.168.1.6     0.000066\n",
      "173.194.187.71   192.168.1.6     1.154031\n",
      "173.194.188.136  192.168.1.6     0.524303\n",
      "173.194.188.72   192.168.1.6     1.606802\n",
      "74.125.110.102   192.168.1.6     2.626355\n",
      "74.125.111.102   192.168.1.6     1.745356\n",
      "74.125.111.106   192.168.1.6     0.505985\n",
      "74.125.153.7     192.168.1.6     1.030191\n",
      "74.125.154.138   192.168.1.6     0.189498\n",
      "74.125.160.202   192.168.1.6     2.884792\n",
      "74.125.160.38    192.168.1.6     0.008204\n",
      "74.125.99.108    192.168.1.6     0.000132\n",
      "74.125.99.168    192.168.1.6     0.410873\n",
      "74.125.99.170    192.168.1.6     2.693258\n",
      "91.81.217.140    192.168.1.6    15.831871\n",
      "91.81.217.141    192.168.1.6     0.005119\n",
      "Name: Length, dtype: float64\n",
      "DOM_ID: ('91.81.217.140', '192.168.1.6')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Captures'\n",
    "tcpdumpfiles = [f for f in listdir(path) if (isfile(join(path, f)) and splitext(join(path,f))[-1] == '.csv')]\n",
    "tcpdumpfiles.sort(key=natural_keys)\n",
    "opt = True ## <=====\n",
    "verbose = True\n",
    "\n",
    "cached_ips = []\n",
    "\n",
    "for f in tcpdumpfiles:\n",
    "    df = pd.read_csv(path+\"/\"+f)\n",
    "    print(f, os.path.getsize(path+'/'+f)/10**6, \"MB\")\n",
    "    \n",
    "    domain_name = 'googlevideo'\n",
    "    uplink, downlink, cached_ips = filter_traffic(df,domain_name, cached_ips, opt=opt)\n",
    "    dom_ul, dom_dl = find_dominant(uplink, downlink, verbose=verbose)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concentrate on the same capture as before: we can see that there is a huge flow towards IPs 91.81.217.140 and 91.81.217.141.\n",
    "\n",
    "The same happens with all the captures.\n",
    "\n",
    "It is possible to notice that this IP was the YouTube IP server in other captures in the previous experiment (for example, in Capture_v2_0)!\n",
    "\n",
    "If we assume that the capture files were created one after the another, I can conclude that the browser does not repeat the DNS lookup because it is already cached and gets content from the previous IP.\n",
    "\n",
    "This is why I implemented the reading of the files in an orderly matter, as to realistically simulate the actual capture, taking into account the flows of DNS requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some details on those IPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.217.132.137 : AS15169 Google LLC\n",
      "\n",
      "173.194.160.200 : AS15169 Google LLC\n",
      "\n",
      "173.194.160.219 : AS15169 Google LLC\n",
      "\n",
      "173.194.182.135 : AS15169 Google LLC\n",
      "\n",
      "173.194.182.138 : AS15169 Google LLC\n",
      "\n",
      "173.194.182.230 : AS15169 Google LLC\n",
      "\n",
      "173.194.187.136 : AS15169 Google LLC\n",
      "\n",
      "173.194.187.71 : AS15169 Google LLC\n",
      "\n",
      "173.194.188.105 : AS15169 Google LLC\n",
      "\n",
      "173.194.188.136 : AS15169 Google LLC\n",
      "\n",
      "173.194.188.230 : AS15169 Google LLC\n",
      "\n",
      "173.194.188.72 : AS15169 Google LLC\n",
      "\n",
      "209.85.226.38 : AS15169 Google LLC\n",
      "\n",
      "74.125.104.103 : AS15169 Google LLC\n",
      "\n",
      "74.125.105.10 : AS15169 Google LLC\n",
      "\n",
      "74.125.110.102 : AS15169 Google LLC\n",
      "\n",
      "74.125.111.102 : AS15169 Google LLC\n",
      "\n",
      "74.125.111.105 : AS15169 Google LLC\n",
      "\n",
      "74.125.111.106 : AS15169 Google LLC\n",
      "\n",
      "74.125.153.11 : AS15169 Google LLC\n",
      "\n",
      "74.125.153.24 : AS15169 Google LLC\n",
      "\n",
      "74.125.153.59 : AS15169 Google LLC\n",
      "\n",
      "74.125.153.7 : AS15169 Google LLC\n",
      "\n",
      "74.125.154.138 : AS15169 Google LLC\n",
      "\n",
      "74.125.160.202 : AS15169 Google LLC\n",
      "\n",
      "74.125.160.38 : AS15169 Google LLC\n",
      "\n",
      "74.125.162.39 : AS15169 Google LLC\n",
      "\n",
      "74.125.162.40 : AS15169 Google LLC\n",
      "\n",
      "74.125.163.138 : AS15169 Google LLC\n",
      "\n",
      "74.125.4.230 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.105 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.106 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.108 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.137 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.166 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.168 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.169 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.170 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.72 : AS15169 Google LLC\n",
      "\n",
      "74.125.99.91 : AS15169 Google LLC\n",
      "\n",
      "91.81.217.140 : AS30722 Vodafone Italia S.p.A.\n",
      "\n",
      "91.81.217.141 : AS30722 Vodafone Italia S.p.A.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cached_ips = np.unique(cached_ips).tolist()\n",
    "\n",
    "for ip in cached_ips:\n",
    "    token = ''\n",
    "    response = urlopen('http://ipinfo.io/'+ip+'/org'+token)\n",
    "    html_content = response.read()\n",
    "    encoding = response.headers.get_content_charset('utf-8')\n",
    "    html_text = html_content.decode(encoding)\n",
    "    print(ip, \":\", html_text)\n",
    "    response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to highline that two of the IPs (the ones with much more traffic according to above) belong to Vodafone network. It makes absolute sense, since it is common for an OTT to provide its servers inside the ISP network to reduce latency and improve performance and avoid the expensive use of transits.\n",
    "\n",
    "Of course not all videos may always be available in the YouTube cache inside Vodafone networks and for them the browser reaches IPs belonging directly to Google network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now recreate the dataset with the *opt* flag set to True.\n",
    "\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture_v2_0.csv: DS: 46 GT: 46\n",
      "Capture_v2_1.csv: DS: 34 GT: 34\n",
      "Capture_v2_2.csv: DS: 38 GT: 38\n",
      "Capture_v2_3.csv: DS: 61 GT: 61\n",
      "Capture_v2_4.csv: DS: 65 GT: 65\n",
      "Capture_v2_5.csv: DS: 66 GT: 66\n",
      "Capture_v2_6.csv: DS: 86 GT: 86\n",
      "Capture_v2_7.csv: DS: 91 GT: 91\n",
      "Capture_v2_8.csv: DS: 26 GT: 26\n",
      "Capture_v2_9.csv: DS: 39 GT: 39\n",
      "Capture_v2_10.csv: DS: 46 GT: 46\n",
      "Capture_v2_11.csv: DS: 45 GT: 45\n",
      "Capture_v2_12.csv: DS: 35 GT: 35\n",
      "Capture_v2_13.csv: DS: 15 GT: 15\n",
      "Capture_v2_14.csv: DS: 14 GT: 14\n",
      "Capture_v2_15.csv: DS: 27 GT: 27\n",
      "Capture_v2_16.csv: DS: 18 GT: 18\n",
      "Capture_v2_17.csv: DS: 25 GT: 25\n",
      "Capture_v2_18.csv: DS: 33 GT: 33\n",
      "Capture_v2_19.csv: DS: 34 GT: 34\n",
      "Capture_v2_20.csv: DS: 32 GT: 32\n",
      "Capture_v2_21.csv: DS: 39 GT: 39\n",
      "\n",
      "\n",
      "\n",
      "X: 826 y:  826\n"
     ]
    }
   ],
   "source": [
    "path = 'Captures'\n",
    "X, y = preprocess_data(path, opt=True, verbose=False, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is definitely bigger. Let's train and test the same regressors as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CACHED\n",
    "X = pd.read_csv('dataset_opt.csv')\n",
    "y = pd.read_csv('groundtruth_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RandomForestRegressor \n",
      "\t\t\t\t 2.415683157315008 342.78866712048347\n",
      "***********************************************************************************\n",
      "\n",
      " MLPRegressor \n",
      "\t\t\t\t 2.4346561884279323 360.18022516716974\n",
      "***********************************************************************************\n",
      "\n",
      " Ridge \n",
      "\t\t\t\t 2.5000762609079596 368.41868049688895\n",
      "***********************************************************************************\n",
      "\n",
      " Lasso \n",
      "\t\t\t\t 2.6283018094399866 404.90921745410253\n",
      "***********************************************************************************\n",
      "\n",
      " ElasticNet \n",
      "\t\t\t\t 2.6283018094399866 404.90921745410253\n",
      "***********************************************************************************\n",
      "\n",
      " ExtraTreesRegressor \n",
      "\t\t\t\t 2.4031457802086678 365.9908501961011\n",
      "***********************************************************************************\n",
      "\n",
      " DecisionTreeRegressor \n",
      "\t\t\t\t 3.3820637160038527 462.34991722810815\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "for reg in regressor_list:\n",
    "    rmse_request_time, rmse_response_vol = train_test_model(reg, kf, X, y)\n",
    "    print(\"\\n\", reg.__class__.__name__, \"\\n\\t\\t\\t\\t\", statistics.mean(rmse_request_time), statistics.mean(rmse_response_vol))\n",
    "    print(\"***********************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain better performance because, with more data, we have reduced the effect of outliers in the smaller dataset.\n",
    "They affected the *Next_Request_Time* more because the flows, albeit from *googlevideo*, were probably not videos but metrics or ads. The best absolute value for this feature comes from the ExtraTrees regressor.\n",
    "\n",
    "As in the previous case, overall, we obtain the best performances using a Random Forest regressor."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2L+Jm98UxoVZ/jMI+dMiq",
   "collapsed_sections": [
    "7DsJgewH_DVk",
    "jMO_nFab_IPu",
    "d45P-M-v_PKm"
   ],
   "name": "Homework_Skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
